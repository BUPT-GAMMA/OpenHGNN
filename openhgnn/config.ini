

[MHGCN]

lr = 0.0001
weight_decay = 0.0005
model_lr = 0.05
hidden_dim = 384
emb_dim = 200
num_layers = 2
max_epoch = 500



[BPHGNN]

seed = 42
lr=0.2
epochs=100
weight_decay=0.000005
hidden_dim=384
out=200
dropout=0
feature="mul"
normalization='AugNormAdj'
degree=2
per=-1
experiment="base-experiment"




[MetaHIN]

input_dir=/openhgnn/dataset/Common_Dataset/
output_dir=/openhgnn/dataset/Common_Dataset/

dataset=dbook
use_cuda= True
file_num= 10  
num_location= 453
num_fea_item= 2
num_publisher =1698
num_fea_user= 1
item_fea_len= 1
embedding_dim= 32
user_embedding_dim= 32
item_embedding_dim= 32
first_fc_hidden_dim= 64
second_fc_hidden_dim= 64
mp_update= 1
local_update= 1
lr= 5e-4
mp_lr= 5e-3
local_lr= 5e-3
batch_size= 32 
num_epoch= 50
seed=13

[FedHGNN]
fea_dim = 64
in_dim = 64
hidden_dim = 64
out_dim = 64
shared_num = 20
path = Data/
num_heads = [2]
eps = 1
num_sample = 0
valid_step = 5
nonlinearity = relu
is_gcn = False
is_attention = False
hetero = True
is_trans = False
is_random_init = True
is_graph = True
local_train_num = 1
agg_mode = add
agg_func = ATTENTION 
lr = 0.01
dropout = 0
weight_decay = 0
epochs = 10000
batch_size = 32
l2_reg = True
grad_limit = 1.0
clients_limit = 0.1
items_limit = 60
type = ATTENTION 
p1 = 1
p2 = 1
seed = 20211111
hidden_size=8
out_size=64


[HGA]

lr = 0.05
weight_decay = 0.001
dropout = 0.6
hidden_dim = 64
patience = 100
mini_batch_flag = true
batch_size = 256
max_epoch = 10
num_layers = 3
num_heads = 8
seed = 0
out_dim = 4
gamma=1.0


[HGMAE]

dataset = acm
in_dim = 1902
hidden_dim = 1024
category = paper
feat_drop = 0.2
attn_drop = 0.5
residual = False
negative_slope = 0.2
num_classes = 3
num_heads = 4
num_layers = 2
num_out_heads = 1

mp_edge_recon_loss_weight = 1.0
mp_edge_mask_rate = 0.7
mp_edge_gamma = 3.0

node_mask_rate = 0.5, 0.005, 0.8

attr_restore_loss_weight = 1.0
attr_restore_gamma = 1.0
attr_replace_rate = 0.2
attr_unchanged_rate = 0.3
mp2vec_negative_size = 3
mp2vec_window_size = 3
mp2vec_batch_size = 256
mp2vec_rw_length = 10
mp2vec_walks_per_node = 3
mp2vec_train_epoch = 20
mp2vec_train_lr = 0.001
mp2vec_feat_dim = 64
mp2vec_feat_pred_loss_weight = 0.1
mp2vec_feat_gamma = 2.0
mp2vec_feat_drop = 0.2

patience = 10
gpu = 0
mae_epochs = 10000
mae_lr = 0.0008
l2_coef = 0
eva_lr = 0.01
eva_wd = 0.0005
scheduler_gamma = 0.999








[HGPrompt]
feats-type = 2
hidden-dim = 64
num-heads = 8
epoch = 300
patience = 30
repeat = 1
model-type = gcn
num-layers = 2
lr = 1e-3
run = 1
device = 1
dropout = 0.5
weight-decay = 1e-6
slope = 0.05
_dataset = ACM
seed = 0
tuple_neg_disconnected_num = 1
tuple_neg_unrelated_num = 1
target_tuple_neg_disconnected_num = 1
subgraph_hop_num = 1
subgraph_neighbor_num_bar = 10
temperature = 1.0
loss_weight = 1.0
hetero_pretrain = 0
target_pretrain = 0
hetero_subgraph = 0
semantic_weight = 0
each_loss = 0
freebase_type = 2
edge_feats = 64



; downstream
feats_type_down = 2  
hidden_dim_down = 64  
bottle_net_hidden_dim_down = 2  
bottle_net_output_dim_down = 64  
edge_feats_down = 64
num_heads_down = 8  
epoch_down = 300  
patience_down = 30  
repeat_down = 1  
model_type_down = gcn 
num_layers_down = 2
lr_down = 1.0
run_down = 1
device_down = 1
dropout_down = 0.5
weight_decay_down = 1e-6
slope_down = 0.05
dataset_down = ACM
seed_down = 0
tasknum_down = 100
shotnum_down = 1
load_pretrain_down = 1
tuning_down = weight-sum-center-fixed
subgraph_hop_num_down = 1
pre_loss_weight_down = 1.0
hetero_pretrain_down = 0
hetero_pretrain_subgraph_down = 0
pretrain_semantic_down = 0
add_edge_info2prompt_down = 1
each_type_subgraph_down = 1
pretrain_each_loss_down = 0
cat_prompt_dim_down = 64 
cat_hprompt_dim_down = 64  
tuple_neg_disconnected_num_down = 1  
tuple_neg_unrelated_num_down = 1  
meta_path_down = 0
semantic_prompt_down = 1
freebase_type_down = 0 
semantic_prompt_weight_down = 0.1
shgn_hidden_dim_down = 3  



;##########################################



[DisenKGAT]
#   str
name = Disen_Model
#   data =  DisenKGAT_WN18RR
#   model = DisenKGAT
score_func = interacte
opn = cross
#  gpu = 2
logdir = ./log/
config = ./config/
strategy = one_to_n
form = plain
mi_method = club_b
att_mode = dot_weight
score_method = dot_rel
score_order = after
gamma_method = norm


#   int
k_w = 10
batch = 2048
test_batch = 2048
epoch = 1500
num_workers = 10
seed = 41504
init_dim = 100
gcn_dim = 200
embed_dim = 200
gcn_layer = 1
k_h = 20
num_filt = 200
ker_sz = 7
num_bases = -1
neg_num = 1000
ik_w = 10
ik_h = 20
inum_filt = 200
iker_sz = 9
iperm = 1
head_num = 1
num_factors = 3
early_stop = 200
mi_epoch = 1

#   float
feat_drop = 0.3
hid_drop2 = 0.3
hid_drop = 0.3
gcn_drop = 0.4
gamma = 9.0
l2 = 0.0
lr = 0.001
lbl_smooth = 0.1
iinp_drop = 0.3
ifeat_drop = 0.4
ihid_drop = 0.3
alpha = 1e-1
max_gamma = 5.0
init_gamma = 9.0

#   boolean
restore = False
bias = False
no_act = False
mi_train = True
no_enc = False
mi_drop = True
fix_gamma = False





[NBF]
input_dim = 32
hidden_dims = [32, 32, 32, 32, 32, 32]
message_func = distmult
aggregate_func = pna
short_cut = True
layer_norm = True
dependent = False
num_negative = 32
strict_negative = True
adversarial_temperature = 1
metric = ['mr', 'mrr', 'hits@1', 'hits@3', 'hits@10', 'hits@10_50']
lr = 0.005
gpus = [0]
batch_size = 64
num_epoch = 20
log_interval = 100


[General]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2
seed = 0
hidden_dim = 64
max_epoch = 50
patience = 200
mini_batch_flag = False

[SHGP]
save_emb = store_true
dataset = mag
target_type = p
train_percent = 0.08
seed = 0
hidden_dim = [256,512]
epochs = 100
lr = 0.0005
l2_coef = 5e-4
type_fusion = att
type_att_size = 64
warm_epochs = 10
compress_ratio = 0.01
cuda = -1


[NSHE]
learning_rate = 0.001
weight_decay = 0.00001
beta = 0.05
seed = 0
norm_emd_flag = True
dropout = 0.5

project_dim = 128
context_dim = 64
hidden_dim = 128
emd_dim = 128

num_e_neg = 1
num_ns_neg = 4
num_heads = 8
max_epoch = 500
patience = 10

optimizer = adam
mini_batch_flag = False

[GTN]
learning_rate = 0.005
weight_decay = 0.001

hidden_dim = 128
out_dim = 16
num_channels = 2
num_layers = 2

seed = 0
max_epoch = 50
patience = 10

identity = True
norm_emd_flag = True
adaptive_lr_flag = True
mini_batch_flag = False

[MHNF]
learning_rate = 0.05
weight_decay = 0.001

;Hidden layer dimension
hidden_dim = 64
;Number of classification type.
out_dim = 16
;Number of conv channels
num_channels = 2
;Length of hybrid metapath
num_layers = 2

seed = 0
max_epoch = 50
patience = 10

;If True, the identity matrix will be added to relation matrix set
identity = False
;If True, the adjacency matrix will be normalized.
norm_emd_flag = True
;If True, the learning rate can be adaptived
adaptive_lr_flag = True
mini_batch_flag = False

[RSHN]
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.2

seed = 1233
;Hidden layer dimension
hidden_dim = 16
max_epoch = 500
rw_len = 4
batch_size = 1000
;Number of Nodes Layers applied
num_node_layer = 2
;Number of Edges Layers applied
num_edge_layer = 2
patience = 50
validation = True
mini_batch_flag = False

[RHGNN]
learning_rate = 0.001
num_heads = 8
hidden_dim = 64
relation_hidden_units = 8
drop_out = 0.5
num_layers = 2
residual = True
batch_size = 80
node_neighbors_min_num = 10
optimizer = adam
weight_decay = 0.0
max_epoch = 100
patience = 50
mini_batch_flag = True
negative_slope = 0.2
norm = True
dropout = 0.2
n_heads = 4
category = movie
out_dim = 3
use_uva = False
fanout = -1

[RGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed = 0
in_dim = 64
hidden_dim = 64
# number of weight matrix bases
n_bases = 40
num_layers = 3

max_epoch = 50
patience = 50
batch_size = 128
fanout = 4

validation = True
use_self_loop = False
mini_batch_flag = True
use_uva = True

[CompGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed = 0
num_layers = 2
in_dim = 32
hidden_dim = 32
out_dim = 32
;We restrict the number of hidden units to 32. from paper

max_epoch = 500
patience = 100
;sub(subtraction) mult(multiplication) ccorr(circular-correlation)
comp_fn = sub
validation = True
mini_batch_flag = True
batch_size = 128
fanout = 4
[HetGNN]
seed = 0
learning_rate = 0.001
weight_decay = 0.00001

dim = 128
max_epoch = 500
batch_size = 64
window_size = 5
num_workers = 4
batches_per_epoch = 50

rw_length = 50
rw_walks = 10
rwr_prob = 0.5

patience = 20
mini_batch_flag = True

[Metapath2vec]
learning_rate = 0.01
# embedding dimension
dim = 128
max_epoch = 1
batch_size = 512
# context window size on a walk trace
window_size = 5
num_workers = 4
# walk length of one random walk
rw_length = 20
# number of random walks per node
rw_walks = 10
# number of negative nodes per positive sample
neg_size = 5
seed = 0
# key of meta path defined in dataset
meta_path_key = APVPA

[HERec]
learning_rate = 0.01
# embedding dimension
dim = 128
max_epoch = 1
batch_size = 128
# context window size on a walk trace
window_size = 2
num_workers = 4
rw_length = 100
# number of random walks per node
rw_walks = 10
# number of negative nodes per positive sample
neg_size = 5
seed = 0
# key of meta path defined in dataset
meta_path_key = APVPA

[HAN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6

hidden_dim = 128
out_dim = 16
; number of attention heads
num_heads = 8
max_epoch = 200
patience = 100
mini_batch_flag = True

[RoHe]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6

hidden_dim = 128
out_dim = 16
; number of attention heads
num_heads = 8
max_epoch = 200
patience = 100
mini_batch_flag = False

[NARS]
seed = 0
learning_rate = 0.003
weight_decay = 0.001
dropout = 0.7
hidden_dim = 64
out_dim = 16
num_heads = 8
num_hops = 2
max_epoch = 200
mini_batch_flag = False
R = 2
patience = 100
input_dropout = True
cpu_preprocess = True
ff_layer = 2

[MAGNN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.3

hidden_dim = 64
out_dim = 3

inter_attn_feats = 128
;The number of attention heads
num_heads = 8
;The number of layers
num_layers = 4

;Maximum number of epoches
max_epoch = 10
patience = 30

;the type of encoder, e.g ['RotateE', 'Average', 'Linear']
encoder_type = RotateE
mini_batch_flag = False
batch_size = 8
num_samples = 5

[HGNN_AC]
feats_drop_rate = 0.2
attn_vec_dim = 32
feats_opt = 110
loss_lambda = 0.2
src_node_type = 2
dropout = 0.1
num_heads = 8
HIN = MAGNN

[HGT]
seed = 0
learning_rate = 0.001
weight_decay = 0.0001
dropout = 0.4

batch_size = 5120
patience = 40
hidden_dim = 64
out_dim = 16
num_layers = 2
num_heads = 8
num_workers = 64
max_epoch = 500
mini_batch_flag = True
fanout = 5
norm = True
use_uva = True

[HeCo]
seed = 2
hidden_dim = 64
max_epoch = 10000
eva_lr = 0.05
eva_wd = 0
patience = 5
learning_rate = 0.0008
weight_decay = 0
tau = 0.8
feat_drop = 0.3
attn_drop = 0.5
sample_rate = author-7_subject-1
lam = 0.5
mini_batch_flag = False

[DMGI]
seed = 0
learning_rate = 0.0005
weight_decay = 0.0001
sc = 3
dropout = 0.5
reg_coef = 0.001
sup_coef = 0.1

patience = 20
hidden_dim = 64
num_heads = 1
max_epoch = 10000
isSemi = False
isBias = False
isAttn = False

[SLiCE]
data_name = amazon
num_walks_per_node = 1
beam_width = 4
max_length = 6
walk_type = dfs
batch_size = 128
outdir = ./openhgnn/output/SLiCE/amazon/
pretrained_embeddings = ./openhgnn/output/SLiCE/amazon/amazon.embed
n_pred = 1
max_pred = 1
lr = 0.0001
n_epochs = 300
get_bert_encoder_embeddings = false
checkpoint = 20
path_option = shortest
ft_batch_size = 100
d_model = 200
ft_d_ff = 512
ft_layer = ffn
ft_drop_rate = 0.1
ft_input_option = last4_cat
ft_lr = 0.00005
ft_n_epochs = 200
num_layers = 6
ft_checkpoint = 1000

[HPN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6
k_layer = 2
alpha = 0.1
edge_drop = 0

hidden_dim = 64
out_dim = 16
max_epoch = 200
patience = 100
mini_batch_flag = False

[KGCN]
seed = 0
weight_decay = 1e-4
lr = 0.002
in_dim = 16
out_dim = 16
batch_size = 128
n_neighbor = 8
aggregate = SUM
n_relation = 60
n_user = 1872
# epoch_iter = 100
max_epoch = 100
mini_batch_flag = True

[HeGAN]
seed = 0
lr_gen = 0.001
lr_dis = 0.001
wd_gen = 1e-5
wd_dis = 1e-5
sigma = 1.0
n_sample = 16
max_epoch = 100
emb_size = 64
epoch_dis = 10
epoch_gen = 5
mini_batch_flag = False
validation = True
patience = 10
label_smooth = 0.05

[general_HGNN]

gnn_type = gcnconv
dropout = 0.5
has_bn = true
activation = tanh
has_l2norm = true

hidden_dim = 64
max_epoch = 400
lr = 0.01

optimizer = Adam
weight_decay = 0.0001
patience = 40

layers_gnn = 4
layers_post_mp = 1
layers_pre_mp = 1
stage_type = stack

macro_func = attention
num_heads = 8
feat = 0
subgraph_extraction = metapath
mini_batch_flag = false

[homo_GNN]

gnn_type = gcnconv
dropout = 0.5
has_bn = true
activation = tanh
has_l2norm = true

hidden_dim = 64
max_epoch = 400
lr = 0.01

optimizer = Adam
weight_decay = 0.0001
patience = 40

layers_gnn = 4
layers_post_mp = 1
layers_pre_mp = 1
stage_type = stack

num_heads = 8
feat = 0
subgraph = metapath
mini_batch_flag = false

[HDE]
emb_dim = 128
num_neighbor = 5
use_bias = true
k_hop = 2
max_epoch = 400
batch_size = 32
max_dist = 3
lr = 0.001

[SimpleHGN]
hidden_dim = 256
num_layers = 3
num_heads = 8
feats_drop_rate = 0.2
slope = 0.05
edge_dim = 64
seed = 0
max_epoch = 500
patience = 100
lr = 0.001
weight_decay = 5e-4
beta = 0.05
residual = True
mini_batch_flag = True
fanout = 5
batch_size = 2048
use_uva = True

[GATNE-T]

learning_rate = 0.01
patience = 2
max_epoch = 5
batch_size = 256
num_workers = 4
dim = 200
edge_dim = 10
att_dim = 20
rw_length = 10
rw_walks = 20
window_size = 5
neg_size = 5
neighbor_samples = 10
score_fn = dot-product

[HetSANN]
lr = 0.0001
weight_decay = 0.0005
dropout = 0.2
seed = 0
hidden_dim = 64
num_layers = 2
num_heads = 16
max_epoch = 10000
patience = 100
slope = 0.2
residual = True
mini_batch_flag = True
batch_size = 2048
fanout = 5
use_uva = True

[ieHGCN]
num_layers = 5
hidden_dim = 64
attn_dim = 32
out_dim = 16
patience = 100
seed = 0
lr = 0.001
weight_decay = 5e-4
max_epoch = 3500
mini_batch_flag = True
fanout = 10
batch_size = 512
dropout = 0.2
bias = True
batchnorm = True

[HGAT]
num_layers = 3
hidden_dim = 64
attn_dim = 32
num_classes = 16
negative_slope = 0.2
patience = 100
seed = 0
lr = 0.01
weight_decay = 5e-4
max_epoch = 350

[HGSL]
# acm4GTN & dblp4GTN
undirected_relations = author-paper,paper-subject
# yelp4HGSL
; undirected_relations = b-l,b-s,b-u
gnn_dropout = 0
fs_eps = 0.8
fp_eps = 0.2
mp_eps = 0.6
hidden_dim = 128
num_heads = 3
gnn_emd_dim = 128
lr = 0.001
weight_decay = 0.0001
max_epoch = 200

[TransE]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
hidden_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transe
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[TransH]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
hidden_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transh
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[TransR]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
ent_dim = 400
rel_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transr
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[TransD]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
ent_dim = 400
rel_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transd
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[GIE]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
hidden_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = gie
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[GIN]
hidden_dim=64
input_dim=64
out_dim=3
num_layers = 2
batch_size=128
lr=0.01
weight_decay=0.0009
mini_batch_flag = True
max_epoch = 350
patience = 100
learn_eps = False
aggregate = sum
fanout = -1

[RGAT]
;Input tensor dimension
in_dim = 64
;The number of layers
num_layers = 3
;The dimension of hidden layers tensor
hidden_dim = 64
patience = 100
seed = 0
lr = 0.01
weight_decay = 5e-4
max_epoch = 350
dropout = 0.2
;The number of attention heads
num_heads = 3
;Output tensor dimension
out_dim = 3

[Rsage]
;Input tensor dimension
in_dim = 64
;The number of layers
num_layers = 3
;The dimension of hidden layers tensor
hidden_dim = 64
patience = 100
seed = 0
lr = 0.01
weight_decay = 5e-4
max_epoch = 350
dropout = 0.2
;The type of aggregator, can be 'pool' or 'mean' or  'lstm' or 'gcn'
aggregator_type = pool
;Output tensor dimension
out_dim = 3

[MG2VEC]
learning_rate = 0.001
max_epoch = 3
dim = 128
batch_size = 512
num_workers = 0
sample_num = 10
alpha = 0.5
seed = 0

[DHNE]
lr = 1e-2
embedding_sizes = 32
prefix_path = 'model'
hidden_size = 64
epochs_to_train = 10
max_epoch = 20
batch_size = 16
alpha = 1
num_neg_samples = 5
seed = 2002
dim_features = [64, 64, 64]

[DiffMG]
lr = 0.01
wd = 0.001
dropout = 0.6
embedding_sizes = 64
max_epoch = 200
hidden_dim = 64
Amazon_train_seed = 1
Amazon_preprocess_seed = 3
Amazon_gen_neg_seed = 4
Amazon_search_seed = 0
attn_dim = 64
use_norm = True
out_nl = True
mini_batch_flag = False
search_lr = 0.01
search_wd = 0.001
search_alr = 3e-4
search_steps_s = 4
search_steps_t = 4
search_epochs = 100
search_eps= 0.5
search_decay = 0.9

[MeiREC]
lr = 0.001
weight_decay = 0.00001
vocab_size = 280000
train_epochs = 25
batch_num = 512
num_workers = 8
val_frequency = 1
save_frequency = 2

[AEHCL]
lr = 0.001
hidden_dim = 64
weight_intra_pair = 1.0
weight_intra_multi = 0.8
weight_inter = 0.2
num_of_attention_heads = 4
t = 1.0
batch_size = 64
weight_decay = 0
eval_epoch = 1
max_epoch = 6
neg_num = 10

[KGAT]
lr = 0.0001
max_epoch = 1000
stopping_steps = 20
;0: No pretrain, 1: Pretrain with the learned embeddings, 2: Pretrain with stored model.
use_pretrain = 1
seed = 2023
;Specify the type of the aggregation layer from {gcn, graphsage, bi-interaction}.
aggregation_type = bi-interaction
;User / entity Embedding size.
entity_dim = 64
;Relation Embedding size.
relation_dim = 64
;Output sizes of every aggregation layer.
conv_dim_list = [64, 32, 16]
;Dropout probability w.r.t. message dropout for each deep layer. 0: no dropout.
mess_dropout = [0.1, 0.1, 0.1]
cf_l2loss_lambda = 1e-5
kg_l2loss_lambda = 1e-5
cf_batch_size = 1024
kg_batch_size = 1024
test_batch_size = 1024
multi_gpu = False
;Calculate metric@K when evaluating.
K = 20

[DSSL]
epochs = 200
lr = 0.001
weight_decay = 1e-3
hidden_channels = 64
num_layers = 2
dropout = 0.5
normalize_features = True
seed = 0
display_step = 25
train_prop = 0.48
valid_prop = 0.32
batch_size = 1024
rand_split = True
embedding_dim = 10
neighbor_max = 5
cluster_num = 6
no_bn = False
alpha = 1
gamma =0.1
entropy = 0.0
tau =0.99
encoder = GCN
mlp_bool = 1
tao = 1
beta = 1
mlp_inference_bool = 1
neg_alpha = 0
load_json = 0

[HGCL]
batch = 8192
epochs = 400
wu1 = 0.8
wu2 = 0.2
wi1 = 0.8
wi2 = 0.2
lr = 0.055
topk = 10
hide_dim = 32
metareg = 0.15
ssl_temp = 0.5
ssl_ureg = 0.04
ssl_ireg = 0.05
ssl_reg = 0.01
ssl_beta = 0.32
rank = 3
Layers = 2
reg = 0.043

[lightGCN]
lr = 0.001
weight_decay = 0.0001
max_epoch = 1000
batch_size = 1024
embedding_size = 64
num_layers = 3
test_u_batch_size = 100
topks = 20

[HMPNN]
lr = 0.001
num_layers = 4
hid_dim = 128
max_epoch = 1000
batch_size = 256
[SeHGNN]
seeds = 1
dataset = ogbn-mag
gpu = 0
cpu = False
root = openhgnn/dataset/data/
stages = 300, 300, 300, 300
emb_path = openhgnn/dataset/data/
extra_embedding = 
embed_size = 256
num_hops = 2
label_feats = True
num_label_hops = 2
hidden = 512
dropout = 0.5
n_layers_1 = 2
n_layers_2 = 2
n_layers_3 = 4
input_drop = 0.1
att_drop = 0.
label_drop = 0.
residual = True
act = leaky_relu
bns = True
label_bns = True
amp = True
lr = 0.001
weight_decay = 0
eval_every = 1
batch_size = 10000
patience = 100
threshold = 0.75
gama = 10
start_stage = 0
reload = 
label_residual = True


[Grail]
num_epochs: 100
eval_every: 3
eval_every_iter: 455
save_every: 10
early_stop: 100
optimizer: Adam
lr: 0.01
clip:1000
l2: 5e-4
margin: 10
max_links:1000000
hop: 3
max_nodes_per_hop: 0
use_kge_embeddings: False
kge_model: TransE
model_type: dgl
constrained_neg_prob: 0.0
batch_size: 16
num_neg_samples_per_link: 1
num_workers: 8
add_traspose_rels: False
enclosing_sub_graph: True
rel_emb_dim: 32
attn_rel_emb_dim: 32
emb_dim: 32
num_gcn_layers: 3
num_bases: 4
dropout: 0
edge_dropout: 0.5
gnn_agg_type: sum
add_ht_emb: True
has_attn: True
mode: sample

[ComPILE]
num_epochs: 100
eval_every: 3
eval_every_iter: 455
save_every: 10
early_stop: 100
optimizer: Adam
lr: 0.01
clip:1000
l2: 5e-4
margin: 10
max_links:1000000
hop: 3
max_nodes_per_hop: 0
use_kge_embeddings: False
kge_model: TransE
model_type: dgl
constrained_neg_prob: 0.0
batch_size: 16
num_neg_samples_per_link: 1
num_workers: 8
add_traspose_rels: False
enclosing_sub_graph: True
rel_emb_dim: 32
attn_rel_emb_dim: 32
emb_dim: 32
num_gcn_layers: 3
num_bases: 4
dropout: 0
edge_dropout: 0.5
gnn_agg_type: sum
add_ht_emb: True
has_attn: True
mode: sample


[AdapropT]
data_path = data/family/
layers=8
sampling=incremental
act=relu
weight=None
tau=1.0
train=True
remove_1hop_edges=False
scheduler=exp
fact_ratio=0.9
epoch=300
eval_interval=1
topk = 100
lr = 0.0036
decay_rate = 0.999
lamb = 0.000017
hidden_dim = 48
attn_dim = 5
dropout = 0.29
n_edge_topk = -1
n_layer = 8
n_batch = 20
n_node_topk = 800
seed = 1234
n_tbatch=20
eval=False
[AdapropI]
data_path=./data/fb237_v1
seed=1234

[LTE]
model_name_GCN=LTE_Transe
model_name=LTE
name=lte
data=FB15k-237
score_func=transe
opn=mult
hid_drop=0.2
gpu=0
x_ops=p
n_layer=0
init_dim=200
batch_size=64
epoch=300
l2=0.0
lr=0.001
lbl_smooth=0.1
num_workers=8
seed=12345
restore=False
bias=False
num_bases=-1
gcn_dim=200
gcn_drop=0.1
conve_hid_drop=0.3
feat_drop=0.2
input_drop=0.2
k_w=20
k_h=10
num_filt=200
ker_sz=7
gamma=9.0
rat=False
wni=False
wsi=False
ss=False
nobn=False
noltr=False
encoder=compgcn
max_epochs=500

[SACN]
seed=12345
init_emb_size=200
gc1_emb_size=150
embedding_dim=200
input_dropout=0
dropout_rate=0.2
channels=200
kernel_size=5
gpu=5
lr=0.002
n_epochs=300
num_workers=2
eval_every=1
dataset_data=FB15k-237
batch_size=64
patience=100
decoder=transe
gamma=9.0
name=repro
n_layer=1
rat=False
wsi=False
wni=False
ss=-1
final_act=True
final_bn=False
final_drop=False

[ExpressGNN]
seed=10
embedding_size = 128
gcn_free_size = 127
slice_dim = 16
no_train = 0
filtered = filtered
hidden_dim = 64
num_epochs = 100
batchsize = 16
trans = 0
num_hops = 2
num_mlp_layers = 2
num_batches = 100
learning_rate = 0.0005
lr_decay_factor = 0.5
lr_decay_patience = 100
lr_decay_min = 0.00001
patience = 20
l2_coef = 0.0
observed_prob = 0.9
entropy_temp = 1
no_entropy = 0
rule_weights_learning = 1
learning_rate_rule_weights = 0.001
epoch_mode = 0
shuffle_sampling = 1
load_method = 1
load_s = 1
use_gcn = 1
filter_latent = 0
closed_world = 0


[Ingram]
margin = 2
lr = 5e-4
nle = 2
nlr = 2
d_e = 32
d_r = 32
hdr_e = 8
hdr_r = 4
num_bin = 10
num_epoch = 10000
validation_epoch = 200
num_head = 8
num_neg = 10


[RedGNN]
seed = 0
patience = 3
batch_size = 100
hidden_dim = 64
optimizer = Adam
lr = 0.005
weight_decay = 0.0002
max_epoch = 50
decay_rate = 0.991
attn_dim = 5
dropout = 0.21
act = idd
n_layer = 5

[RedGNNT]
seed = 0
patience = 3
batch_size = 20
hidden_dim = 48
optimizer = Adam
lr = 0.0036
weight_decay = 0.000017
max_epoch = 50
decay_rate = 0.999
attn_dim = 5
dropout = 0.21
act = relu
n_layer = 3
n_tbatch = 50

[SIAN]
user_num = 8163
item_num = 7900
profile_size = 150
batch_size = 16
emb_size = 64
lr = 0.001
weight_decay = 5e-4
test_batch_size = 16
epochs = 100
eval_num = 100
top_k = 10
worker_num = 0
seed = 72



[RHINE]
seed = 0
lr = 0.005
weight_decay = 0.001
max_epoch = 100
patience = 100
mini_batch_flag = True
emb_dim=20
hid_dim=64
batch_size=128
