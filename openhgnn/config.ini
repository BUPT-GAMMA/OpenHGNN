[NSHE]
learning_rate = 0.001
weight_decay = 0.00001
beta = 0.05
seed = 0
norm_emd_flag = True

project_dim=128
context_dim=64
emd_dim=128

num_e_neg = 1
num_ns_neg = 4
max_epoch = 500
patience = 10

optimizer = adam
mini_batch_flag = False
[GTN]
learning_rate = 0.005
weight_decay = 0.001

hidden_dim = 128
out_dim = 16
num_channels = 2
num_layers = 2

seed = 0
max_epoch = 50
patience = 10

identity = True
norm_emd_flag = True
adaptive_lr_flag = True
mini_batch_flag = False

[MHNF]
learning_rate = 0.05
weight_decay = 0.001

hidden_dim = 64
out_dim = 16
num_channels = 2
num_layers = 2

seed = 0
max_epoch = 50
patience = 10

identity = False
norm_emd_flag = True
adaptive_lr_flag = True
mini_batch_flag = False

[RSHN]
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.2

seed = 1233
hidden_dim = 16
max_epoch = 500
rw_len = 4
batch_size = 1000
num_node_layer = 2
num_edge_layer = 2
patience = 50
validation = True
mini_batch_flag = False

[RHGNN]
learning_rate = 0.001
num_heads = 8
hidden_dim = 64
relation_hidden_units = 8
drop_out = 0.5
n_layers = 2
residual = True
batch_size = 80
node_neighbors_min_num = 10
optimizer = adam
weight_decay = 0.0
max_epoch = 1
patience = 50
mini_batch_flag = True
negative_slope=0.2
norm = True
dropout = 0.2
n_heads = 4
category = movie
out_dim = 3

[RGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed =0
in_dim = 64
hidden_dim = 64
out_dim = 64
n_bases = 40
n_layers = 3

max_epoch = 50
patience = 50
batch_size = 126
fanout = 4

validation = True
use_self_loop = False
mini_batch_flag = False

[CompGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed = 0
n_layers = 2
in_dim = 32
hidden_dim = 32
out_dim = 32
;We restrict the number of hidden units to 32. from paper

max_epoch = 500
patience = 100

comp_fn = sub
validation = True
mini_batch_flag = False

[HetGNN]
seed = 0
learning_rate = 0.001
weight_decay = 0.00001

dim = 128
max_epoch = 500
batch_size = 64
window_size = 5
num_workers = 4
batches_per_epoch = 50

rw_length = 50
rw_walks = 10
rwr_prob = 0.5

patience = 20
mini_batch_flag = True

[Metapath2vec]
learning_rate = 0.01
weight_decay = 0.0001
dim = 128
max_epoch = 100
batch_size = 32
window_size = 5
num_workers = 8
batches_per_epoch = 20

rw_length = 100
rw_walks = 50
rwr_prob = 0.5

seed = 0
patience = 100
mini_batch_flag = True

[HAN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6

hidden_dim = 128
out_dim = 16
num_heads = 8
max_epoch = 200
patience = 100
mini_batch_flag = False

[NARS]
seed = 0
learning_rate = 0.003
weight_decay = 0.001
dropout = 0.7
hidden_dim = 64
out_dim = 16
num_heads = 8
num_hops = 2
max_epoch = 200
mini_batch_flag = False
R = 2
patience = 100
input_dropout = True
cpu_preprocess = True
ff_layer = 2

[MAGNN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001 
dropout = 0.3 

h_dim = 32
out_dim = 3
			 
inter_attn_feats = 64
num_heads = 8 
num_layers = 4 

max_epoch = 100
patience = 30
mini_batch_flag = False
encoder_type = RotateE

[HGNN_AC]
feats_drop_rate = 0.2
attn_vec_dim = 32
feats_opt = 110
loss_lambda = 0.2
src_node_type = 2
dropout = 0.1
num_heads = 8
HIN = MAGNN

[HGT]
seed = 0
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.4

batch_size = 5120
patience =40
in_dim = 32
hidden_dim = 64
out_dim = 16
n_layers = 2
num_heads = 2
num_workers = 64
max_epoch = 200
mini_batch_flag = False

[HeCo]
seed = 2
hidden_dim = 64
max_epoch = 10000
eva_lr = 0.05
eva_wd = 0
patience = 5
learning_rate = 0.0008
weight_decay = 0
tau = 0.8
feat_drop = 0.3
attn_drop = 0.5
sample_rate = author-7_subject-1
lam = 0.5
mini_batch_flag = False

[DMGI]
seed = 0
learning_rate = 0.0005
l2_coef = 0.0001
sc = 3
dropout = 0.5
reg_coef = 0.001
sup_coef = 0.1

patience =20
hid_unit = 64
num_heads = 1
max_epoch = 10000
isSemi = False
isBias = False
isAttn = False

[HPN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6
out_embedsize = 64
k_layer = 2
alpha = 0.1
edge_drop = 0

hidden_dim = 64
out_dim = 16
max_epoch = 200
patience = 100
mini_batch_flag = False

[KGCN]
seed = 0
weight_decay = 1e-4
lr = 0.002
in_dim = 16
out_dim = 16
batch_size = 128
n_neighbor = 8
aggregate = SUM
n_relation = 60
n_user = 1872
epoch_iter = 100
mini_batch_flag = True

[HeGAN]
seed = 0
lr_gen = 0.001
lr_dis = 0.001
wd_gen = 1e-5
wd_dis = 1e-5
sigma = 1.0
n_sample = 16
max_epoch = 100
emb_size = 64
epoch_dis = 10
epoch_gen = 5
mini_batch_flag = False
validation = True
patience = 10

[general_HGNN]

gnn_type = gcnconv
dropout = 0.5
has_bn = true
activation = tanh
has_l2norm = true

hidden_dim = 64
max_epoch = 400
lr = 0.01

optimizer = Adam
weight_decay = 0.0001
patience = 40

layers_gnn = 4
layers_post_mp = 1
layers_pre_mp = 1
stage_type = stack

macro_func = attention
num_heads = 8
feat = 0
subgraph=metapath
mini_batch_flag = false

[homo_GNN]

gnn_type = gcnconv
dropout = 0.5
has_bn = true
activation = tanh
has_l2norm = true

hidden_dim = 64
max_epoch = 400
lr = 0.01

optimizer = Adam
weight_decay = 0.0001
patience = 40

layers_gnn = 4
layers_post_mp = 1
layers_pre_mp = 1
stage_type = stack

num_heads = 8
feat = 0
subgraph=metapath
mini_batch_flag = false