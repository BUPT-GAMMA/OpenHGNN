[General]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2
seed = 0
hidden_dim = 64
max_epoch = 50
patience = 200
mini_batch_flag = False

[NSHE]
learning_rate = 0.001
weight_decay = 0.00001
beta = 0.05
seed = 0
norm_emd_flag = True

project_dim = 128
context_dim = 64
emd_dim = 128

num_e_neg = 1
num_ns_neg = 4
max_epoch = 500
patience = 10

optimizer = adam
mini_batch_flag = False
[GTN]
learning_rate = 0.005
weight_decay = 0.001

hidden_dim = 128
out_dim = 16
num_channels = 2
num_layers = 2

seed = 0
max_epoch = 50
patience = 10

identity = True
norm_emd_flag = True
adaptive_lr_flag = True
mini_batch_flag = False

[MHNF]
learning_rate = 0.05
weight_decay = 0.001

;Hidden layer dimension
hidden_dim = 64
;Number of classification type.
out_dim = 16
;Number of conv channels
num_channels = 2
;Length of hybrid metapath
num_layers = 2

seed = 0
max_epoch = 50
patience = 10

;If True, the identity matrix will be added to relation matrix set
identity = False
;If True, the adjacency matrix will be normalized.
norm_emd_flag = True
;If True, the learning rate can be adaptived
adaptive_lr_flag = True
mini_batch_flag = False

[RSHN]
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.2

seed = 1233
;Hidden layer dimension
hidden_dim = 16
max_epoch = 500
rw_len = 4
batch_size = 1000
;Number of Nodes Layers applied
num_node_layer = 2
;Number of Edges Layers applied
num_edge_layer = 2
patience = 50
validation = True
mini_batch_flag = False

[RHGNN]
learning_rate = 0.001
num_heads = 8
hidden_dim = 64
relation_hidden_units = 8
drop_out = 0.5
num_layers = 2
residual = True
batch_size = 80
node_neighbors_min_num = 10
optimizer = adam
weight_decay = 0.0
max_epoch = 100
patience = 50
mini_batch_flag = True
negative_slope = 0.2
norm = True
dropout = 0.2
n_heads = 4
category = movie
out_dim = 3
use_uva = False
fanout = -1

[RGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed = 0
in_dim = 64
hidden_dim = 64
# number of weight matrix bases
n_bases = 40
num_layers = 3

max_epoch = 50
patience = 50
batch_size = 128
fanout = 4

validation = True
use_self_loop = False
mini_batch_flag = True
use_uva = True

[CompGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed = 0
num_layers = 2
in_dim = 32
hidden_dim = 32
out_dim = 32
;We restrict the number of hidden units to 32. from paper

max_epoch = 500
patience = 100
;sub(subtraction) mult(multiplication) ccorr(circular-correlation)
comp_fn = sub
validation = True
mini_batch_flag = True
batch_size = 128
fanout = 4
[HetGNN]
seed = 0
learning_rate = 0.001
weight_decay = 0.00001

dim = 128
max_epoch = 500
batch_size = 64
window_size = 5
num_workers = 4
batches_per_epoch = 50

rw_length = 50
rw_walks = 10
rwr_prob = 0.5

patience = 20
mini_batch_flag = True

[Metapath2vec]
learning_rate = 0.01
# embedding dimension
dim = 128
max_epoch = 1
batch_size = 512
# context window size on a walk trace
window_size = 5
num_workers = 4
# walk length of one random walk
rw_length = 20
# number of random walks per node
rw_walks = 10
# number of negative nodes per positive sample
neg_size = 5
seed = 0
# key of meta path defined in dataset
meta_path_key = APVPA

[HERec]
learning_rate = 0.01
# embedding dimension
dim = 128
max_epoch = 1
batch_size = 128
# context window size on a walk trace
window_size = 2
num_workers = 4
rw_length = 100
# number of random walks per node
rw_walks = 10
# number of negative nodes per positive sample
neg_size = 5
seed = 0
# key of meta path defined in dataset
meta_path_key = APVPA

[HAN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6

hidden_dim = 128
out_dim = 16
; number of attention heads
num_heads = 8
max_epoch = 200
patience = 100
mini_batch_flag = False

[NARS]
seed = 0
learning_rate = 0.003
weight_decay = 0.001
dropout = 0.7
hidden_dim = 64
out_dim = 16
num_heads = 8
num_hops = 2
max_epoch = 200
mini_batch_flag = False
R = 2
patience = 100
input_dropout = True
cpu_preprocess = True
ff_layer = 2

[MAGNN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.3

hidden_dim = 64
out_dim = 3

inter_attn_feats = 128
;The number of attention heads
num_heads = 8
;The number of layers
num_layers = 4

;Maximum number of epoches
max_epoch = 10
patience = 30

;the type of encoder, e.g ['RotateE', 'Average', 'Linear']
encoder_type = RotateE
mini_batch_flag = False
batch_size = 8
num_samples = 5

[HGNN_AC]
feats_drop_rate = 0.2
attn_vec_dim = 32
feats_opt = 110
loss_lambda = 0.2
src_node_type = 2
dropout = 0.1
num_heads = 8
HIN = MAGNN

[HGT]
seed = 0
learning_rate = 0.001
weight_decay = 0.0001
dropout = 0.4

batch_size = 5120
patience = 40
hidden_dim = 64
out_dim = 16
num_layers = 2
num_heads = 8
num_workers = 64
max_epoch = 500
mini_batch_flag = True
fanout = 5
norm = True
use_uva = True

[HeCo]
seed = 2
hidden_dim = 64
max_epoch = 10000
eva_lr = 0.05
eva_wd = 0
patience = 5
learning_rate = 0.0008
weight_decay = 0
tau = 0.8
feat_drop = 0.3
attn_drop = 0.5
sample_rate = author-7_subject-1
lam = 0.5
mini_batch_flag = False

[DMGI]
seed = 0
learning_rate = 0.0005
weight_decay = 0.0001
sc = 3
dropout = 0.5
reg_coef = 0.001
sup_coef = 0.1

patience = 20
hidden_dim = 64
num_heads = 1
max_epoch = 10000
isSemi = False
isBias = False
isAttn = False

[SLiCE]
data_name = amazon
num_walks_per_node = 1
beam_width = 4
max_length = 6
walk_type = dfs
batch_size = 128
outdir = ./openhgnn/output/SLiCE/amazon/
pretrained_embeddings = ./openhgnn/output/SLiCE/amazon/amazon.embed
n_pred = 1
max_pred = 1
lr = 0.0001
n_epochs = 300
get_bert_encoder_embeddings = False
checkpoint = 20
path_option = shortest
ft_batch_size = 100
d_model = 200
ft_d_ff = 512
ft_layer = ffn
ft_drop_rate = 0.1
ft_input_option = last4_cat
ft_lr = 0.00005
ft_n_epochs = 200
num_layers = 6
ft_checkpoint = 1000

[HPN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6
k_layer = 2
alpha = 0.1
edge_drop = 0

hidden_dim = 64
out_dim = 16
max_epoch = 200
patience = 100
mini_batch_flag = False

[KGCN]
seed = 0
weight_decay = 1e-4
lr = 0.002
in_dim = 16
out_dim = 16
batch_size = 128
n_neighbor = 8
aggregate = SUM
n_relation = 60
n_user = 1872
epoch_iter = 100
mini_batch_flag = True

[HeGAN]
seed = 0
lr_gen = 0.001
lr_dis = 0.001
wd_gen = 1e-5
wd_dis = 1e-5
sigma = 1.0
n_sample = 16
max_epoch = 100
emb_size = 64
epoch_dis = 10
epoch_gen = 5
mini_batch_flag = False
validation = True
patience = 10
label_smooth = 0.05

[general_HGNN]

gnn_type = gcnconv
dropout = 0.5
has_bn = true
activation = tanh
has_l2norm = true

hidden_dim = 64
max_epoch = 400
lr = 0.01

optimizer = Adam
weight_decay = 0.0001
patience = 40

layers_gnn = 4
layers_post_mp = 1
layers_pre_mp = 1
stage_type = stack

macro_func = attention
num_heads = 8
feat = 0
subgraph_extraction = metapath
mini_batch_flag = false

[homo_GNN]

gnn_type = gcnconv
dropout = 0.5
has_bn = true
activation = tanh
has_l2norm = true

hidden_dim = 64
max_epoch = 400
lr = 0.01

optimizer = Adam
weight_decay = 0.0001
patience = 40

layers_gnn = 4
layers_post_mp = 1
layers_pre_mp = 1
stage_type = stack

num_heads = 8
feat = 0
subgraph = metapath
mini_batch_flag = false

[HDE]
emb_dim = 128
num_neighbor = 5
use_bias = true
k_hop = 2
max_epoch = 400
batch_size = 32
max_dist = 3
lr = 0.001

[SimpleHGN]
hidden_dim = 256
num_layers = 3
num_heads = 8
feats_drop_rate = 0.2
slope = 0.05
edge_dim = 64
seed = 0
max_epoch = 500
patience = 100
lr = 0.001
weight_decay = 5e-4
beta = 0.05
residual = True
mini_batch_flag = True
fanout = 5
batch_size = 2048
use_uva = True

[GATNE-T]

learning_rate = 0.01
patience = 2
max_epoch = 5
batch_size = 256
num_workers = 4
dim = 200
edge_dim = 10
att_dim = 20
rw_length = 10
rw_walks = 20
window_size = 5
neg_size = 5
neighbor_samples = 10
score_fn = dot-product

[HetSANN]
lr = 0.0001
weight_decay = 0.0005
dropout = 0.2
seed = 0
hidden_dim = 64
num_layers = 2
num_heads = 16
max_epoch = 10000
patience = 100
slope = 0.2
residual = True
mini_batch_flag = True
batch_size = 2048
fanout = 5
use_uva = True

[ieHGCN]
num_layers = 5
hidden_dim = 64
attn_dim = 32
out_dim = 16
patience = 100
seed = 0
lr = 0.001
weight_decay = 5e-4
max_epoch = 3500
mini_batch_flag = True
fanout = 10
batch_size = 512
dropout = 0.2
bias = True
batchnorm = True

[HGAT]
num_layers = 3
hidden_dim = 64
attn_dim = 32
num_classes = 16
negative_slope = 0.2
patience = 100
seed = 0
lr = 0.01
weight_decay = 5e-4
max_epoch = 350

[HGSL]
# acm4GTN & dblp4GTN
undirected_relations = author-paper,paper-subject
# yelp4HGSL
; undirected_relations = b-l,b-s,b-u
gnn_dropout = 0
fs_eps = 0.8
fp_eps = 0.2
mp_eps = 0.6
hidden_dim = 128
num_heads = 3
gnn_emd_dim = 128
lr = 0.001
weight_decay = 0.0001
max_epoch = 200

[TransE]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
hidden_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transe
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[TransH]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
hidden_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transh
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[TransR]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
ent_dim = 400
rel_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transr
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[TransD]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
ent_dim = 400
rel_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transd
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[GIE]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
hidden_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = gie
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[GIN]
hidden_dim=64
input_dim=64
out_dim=3
num_layers = 2
batch_size=128
lr=0.01
weight_decay=0.0009
mini_batch_flag = True
max_epoch = 350
patience = 100
learn_eps = False
aggregate = sum
fanout = -1

[RGAT]
;Input tensor dimension
in_dim = 64
;The number of layers
num_layers = 3
;The dimension of hidden layers tensor
hidden_dim = 64
patience = 100
seed = 0
lr = 0.01
weight_decay = 5e-4
max_epoch = 350
dropout = 0.2
;The number of attention heads
num_heads = 3
;Output tensor dimension
out_dim = 3

[Rsage]
;Input tensor dimension
in_dim = 64
;The number of layers
num_layers = 3
;The dimension of hidden layers tensor
hidden_dim = 64
patience = 100
seed = 0
lr = 0.01
weight_decay = 5e-4
max_epoch = 350
dropout = 0.2
;The type of aggregator, can be 'pool' or 'mean' or  'lstm' or 'gcn'
aggregator_type = pool
;Output tensor dimension
out_dim = 3

[MG2VEC]
learning_rate = 0.001
max_epoch = 3
dim = 128
batch_size = 512
num_workers = 0
sample_num = 10
alpha = 0.5
seed = 0

[DHNE]
lr = 1e-2
embedding_sizes = 32
prefix_path = 'model'
hidden_size = 64
epochs_to_train = 10
max_epoch = 20
batch_size = 16
alpha = 1
num_neg_samples = 5
seed = 2002
dim_features = [64, 64, 64]

[DiffMG]
lr = 0.01
wd = 0.001
dropout = 0.6
embedding_sizes = 64
max_epoch = 200
hidden_dim = 64
Amazon_train_seed = 1
Amazon_preprocess_seed = 3
Amazon_gen_neg_seed = 4
Amazon_search_seed = 0
attn_dim = 64
use_norm = True
out_nl = True
mini_batch_flag = False
search_lr = 0.01
search_wd = 0.001
search_alr = 3e-4
search_steps_s = 4
search_steps_t = 4
search_epochs = 100
search_eps= 0.5
search_decay = 0.9

[MeiREC]
lr = 0.001
weight_decay = 0.00001
vocab_size = 280000
train_epochs = 25
batch_num = 512
num_workers = 8
val_frequency = 1
save_frequency = 2