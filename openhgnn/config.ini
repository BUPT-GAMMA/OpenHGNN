[General]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2
seed =0
hidden_dim = 64
max_epoch = 50
patience = 50
mini_batch_flag = False

[NSHE]
learning_rate = 0.001
weight_decay = 0.00001
beta = 0.05
seed = 0
norm_emd_flag = True

project_dim=128
context_dim=64
emd_dim=128

num_e_neg = 1
num_ns_neg = 4
max_epoch = 500
patience = 10

optimizer = adam
mini_batch_flag = False
[GTN]
learning_rate = 0.005
weight_decay = 0.001

hidden_dim = 128
out_dim = 16
num_channels = 2
num_layers = 2

seed = 0
max_epoch = 50
patience = 10

identity = True
norm_emd_flag = True
adaptive_lr_flag = True
mini_batch_flag = False

[MHNF]
learning_rate = 0.05
weight_decay = 0.001

hidden_dim = 64
out_dim = 16
num_channels = 2
num_layers = 2

seed = 0
max_epoch = 50
patience = 10

identity = False
norm_emd_flag = True
adaptive_lr_flag = True
mini_batch_flag = False

[RSHN]
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.2

seed = 1233
hidden_dim = 16
max_epoch = 500
rw_len = 4
batch_size = 1000
num_node_layer = 2
num_edge_layer = 2
patience = 50
validation = True
mini_batch_flag = False

[RHGNN]
learning_rate = 0.001
num_heads = 8
hidden_dim = 64
relation_hidden_units = 8
drop_out = 0.5
num_layers = 2
residual = True
batch_size = 80
node_neighbors_min_num = 10
optimizer = adam
weight_decay = 0.0
max_epoch = 100
patience = 50
mini_batch_flag = True
negative_slope=0.2
norm = True
dropout = 0.2
n_heads = 4
category = movie
out_dim = 3

[RGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed =0
in_dim = 64
hidden_dim = 64
n_bases = 40
num_layers = 3

max_epoch = 50
patience = 50
batch_size = 128
fanout = 4

validation = True
use_self_loop = False
mini_batch_flag = True

[CompGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed = 0
num_layers = 2
in_dim = 32
hidden_dim = 32
out_dim = 32
;We restrict the number of hidden units to 32. from paper

max_epoch = 500
patience = 100

comp_fn = sub
validation = True
mini_batch_flag = False

[HetGNN]
seed = 0
learning_rate = 0.001
weight_decay = 0.00001

dim = 128
max_epoch = 500
batch_size = 64
window_size = 5
num_workers = 4
batches_per_epoch = 50

rw_length = 50
rw_walks = 10
rwr_prob = 0.5

patience = 20
mini_batch_flag = True

[Metapath2vec]
learning_rate = 0.01
dim = 128
max_epoch = 1
batch_size = 512
window_size = 5
num_workers = 4
rw_length = 20
rw_walks = 10
neg_size = 5
seed = 0
meta_path_key = APVPA

[HERec]
learning_rate = 0.01
dim = 128
max_epoch = 1
batch_size = 128
window_size = 2
num_workers = 4
rw_length = 100
rw_walks = 10
neg_size = 5
seed = 0
meta_path_key = APVPA

[HAN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6

hidden_dim = 128
out_dim = 16
num_heads = 8
max_epoch = 200
patience = 100
mini_batch_flag = False

[NARS]
seed = 0
learning_rate = 0.003
weight_decay = 0.001
dropout = 0.7
hidden_dim = 64
out_dim = 16
num_heads = 8
num_hops = 2
max_epoch = 200
mini_batch_flag = False
R = 2
patience = 100
input_dropout = True
cpu_preprocess = True
ff_layer = 2

[MAGNN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001 
dropout = 0.3 


h_dim = 64
out_dim = 3 
			 
inter_attn_feats = 128
num_heads = 8
num_layers = 4

max_epoch = 10
patience = 30

encoder_type = RotateE
mini_batch_flag = False
batch_size = 8
num_samples = 5

[HGNN_AC]
feats_drop_rate = 0.2
attn_vec_dim = 32
feats_opt = 110
loss_lambda = 0.2
src_node_type = 2
dropout = 0.1
num_heads = 8
HIN = MAGNN

[HGT]
seed = 0
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.4

batch_size = 5120
patience =40
hidden_dim = 64
out_dim = 16
num_layers = 2
num_heads = 2
num_workers = 64
max_epoch = 200
mini_batch_flag = False
norm = True

[HeCo]
seed = 2
hidden_dim = 64
max_epoch = 10000
eva_lr = 0.05
eva_wd = 0
patience = 5
learning_rate = 0.0008
weight_decay = 0
tau = 0.8
feat_drop = 0.3
attn_drop = 0.5
sample_rate = author-7_subject-1
lam = 0.5
mini_batch_flag = False

[DMGI]
seed = 0
learning_rate = 0.0005
weight_decay = 0.0001
sc = 3
dropout = 0.5
reg_coef = 0.001
sup_coef = 0.1

patience =20
hidden_dim = 64
num_heads = 1
max_epoch = 10000
isSemi = False
isBias = False
isAttn = False

[SLiCE]
data_name = amazon
num_walks_per_node = 1
beam_width = 4
max_length = 6
walk_type = dfs
batch_size = 128
outdir = ./openhgnn/output/SLiCE/amazon/
pretrained_embeddings = ./openhgnn/output/SLiCE/amazon/amazon.embed
n_pred = 1
max_pred = 1
lr = 0.0001
n_epochs = 300
get_bert_encoder_embeddings = False
checkpoint = 20
path_option = shortest
ft_batch_size = 100
d_model = 200
ft_d_ff = 512
ft_layer = ffn
ft_drop_rate = 0.1
ft_input_option = last4_cat
ft_lr = 0.00005
ft_n_epochs = 200
num_layers = 6
ft_checkpoint = 1000

[HPN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6
k_layer = 2
alpha = 0.1
edge_drop = 0

hidden_dim = 64
out_dim = 16
max_epoch = 200
patience = 100
mini_batch_flag = False

[KGCN]
seed = 0
weight_decay = 1e-4
lr = 0.002
in_dim = 16
out_dim = 16
batch_size = 128
n_neighbor = 8
aggregate = SUM
n_relation = 60
n_user = 1872
epoch_iter = 100
mini_batch_flag = True

[HeGAN]
seed = 0
lr_gen = 0.001
lr_dis = 0.001
wd_gen = 1e-5
wd_dis = 1e-5
sigma = 1.0
n_sample = 16
max_epoch = 100
emb_size = 64
epoch_dis = 10
epoch_gen = 5
mini_batch_flag = False
validation = True
patience = 10
label_smooth=0.05

[general_HGNN]

gnn_type = gcnconv
dropout = 0.5
has_bn = true
activation = tanh
has_l2norm = true

hidden_dim = 64
max_epoch = 400
lr = 0.01

optimizer = Adam
weight_decay = 0.0001
patience = 40

layers_gnn = 4
layers_post_mp = 1
layers_pre_mp = 1
stage_type = stack

macro_func = attention
num_heads = 8
feat = 0
subgraph_extraction = metapath
mini_batch_flag = false

[homo_GNN]

gnn_type = gcnconv
dropout = 0.5
has_bn = true
activation = tanh
has_l2norm = true

hidden_dim = 64
max_epoch = 400
lr = 0.01

optimizer = Adam
weight_decay = 0.0001
patience = 40

layers_gnn = 4
layers_post_mp = 1
layers_pre_mp = 1
stage_type = stack

num_heads = 8
feat = 0
subgraph=metapath
mini_batch_flag = false

[HDE]
emb_dim = 128
num_neighbor = 5
use_bias = true
k_hop = 2
max_epoch = 400
batch_size = 32
max_dist = 3
lr = 0.001

[SimpleHGN]
h_dim = 64
num_layers = 3
num_heads = 8
feats_drop_rate = 0.2
slope = 0.05
edge_dim = 64
seed = 0
max_epoch = 500
patience = 100
lr = 0.001
weight_decay = 5e-4
beta = 0.05
residual = True

[GATNE-T]

learning_rate = 0.01
patience = 2
max_epoch = 5
batch_size = 256
num_workers = 4
dim = 200
edge_dim = 10
att_dim = 20
rw_length = 10
rw_walks = 20
window_size = 5
neg_size = 5
neighbor_samples = 10
score_fn = dot-product

[HetSANN]
lr = 0.0005
weight_decay = 0.0005
dropout = 0.2
seed =0
h_dim = 64
num_layers = 4
num_heads = 16
max_epoch = 10000
patience = 100
slope = 0.2
residual = True

[ieHGCN]
in_dim = 64
num_layers = 5
hidden_dim = 64
attn_dim = 32
out_dim = 16
patience = 100
seed = 0
lr = 0.001
weight_decay = 5e-4
max_epoch = 3500

[HGAT]
in_dim = 64
num_layers = 3
hidden_dim = 64
attn_dim = 32
num_classes = 16
negative_slope = 0.2
patience = 100
seed = 0
lr = 0.01
weight_decay = 5e-4
max_epoch = 350

[TransE]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
hidden_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transe
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[TransH]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
hidden_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transh
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[TransR]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
ent_dim = 400
rel_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transr
filtered = filtered
valid_percent = 0.01
test_percent = 0.1

[TransD]
seed = 0
patience = 3
batch_size = 100
neg_size = 13
dis_norm = 1
margin = 4
ent_dim = 400
rel_dim = 400
optimizer = SGD
lr = 1
weight_decay = 0.0001
max_epoch = 50
score_fn = transd
filtered = filtered
valid_percent = 0.01
test_percent = 0.1
