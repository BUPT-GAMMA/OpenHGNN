import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl.function as fn
from . import BaseModel, register_model


@register_model('ieHGCN')
class ieHGCN(BaseModel):
    r"""
    Description
    -----------
    ie-HGCN from paper `Interpretable and Efficient Heterogeneous Graph Convolutional Network
    <https://arxiv.org/pdf/2005.13183.pdf>`__.

    `Source Code Link <https://github.com/kepsail/ie-HGCN>`_
    """
    @classmethod
    def build_model_from_args(cls, args, hg:dgl.DGLGraph):
        device = hg.device
        # Set up new model's ft_dict
        ft_dict = hg.ndata['h']
        # Set up new model's adj_dict
        adj_dict = {}
        for i in hg.etypes:
            x, y = i.split("-", 2)
            if x not in adj_dict:
                adj_dict[x] = {}
            adj_dict[x][y] = hg.adj(etype=i)
        for k in ft_dict:
            ft_dict[k] = ft_dict[k].to(device)
        for k in adj_dict:
            for kk in adj_dict[k]:
                adj_dict[k][kk] = adj_dict[k][kk].to(device)
        args.hid_layer_dim = [64, 32, 16, 8]
        args.output_layer_dim = 3
        args.type_fusion = 'att'
        args.type_att_size = 64
        label = hg.ntypes
        net_schema = dict([(k, list(adj_dict[k].keys())) for k in adj_dict.keys()])
        layer_shape = []
        input_layer_shape = dict([(k, ft_dict[k].shape[1]) for k in ft_dict.keys()])
        hidden_layer_shape = [dict.fromkeys(ft_dict.keys(), l_hid) for l_hid in args.hid_layer_dim]
        layer_shape.append(input_layer_shape)
        layer_shape.extend(hidden_layer_shape)
        return cls(net_schema=net_schema,
                   layer_shape=layer_shape,
                   label_keys=label,
                   type_fusion=args.type_fusion,
                   type_att_size=args.type_att_size,
                   ft_dict = ft_dict,
                   )

    def __init__(self, net_schema, layer_shape, label_keys, type_fusion, type_att_size,ft_dict):
        super(ieHGCN, self).__init__()
        self.ft_dict = ft_dict
        self.gat_layers = nn.ModuleList()
        for i in range(0,len(layer_shape)-2):
            self.gat_layers.append(
                HeteGCNLayer(
                    net_schema,
                    layer_shape[i],
                    layer_shape[i+1],
                    type_fusion,
                    type_att_size
                )
            )

        self.embd2class = nn.ParameterDict()
        self.bias = nn.ParameterDict()
        self.label_keys = label_keys
        for k in label_keys:
            self.embd2class[k] = nn.Parameter(torch.FloatTensor(layer_shape[-2][k], layer_shape[-1][k]))
            nn.init.xavier_uniform_(self.embd2class[k].data, gain=1.414)
            self.bias[k] = nn.Parameter(torch.FloatTensor(1, layer_shape[-1][k]))
            nn.init.xavier_uniform_(self.bias[k].data, gain=1.414)
        return

    def forward(self, hg, h_dict):
        for i in range(len(self.gat_layers)):
            if i == 0:
                x_dict = self.gat_layers[i](hg, self.ft_dict)
                x_dict = self.non_linear(x_dict)
                x_dict = self.dropout_ft(x_dict,0.5)

            elif 0 < i < len(self.gat_layers)-1:
                x_dict = self.gat_layers[i](hg, x_dict)
                x_dict = self.non_linear(x_dict)
                x_dict = self.dropout_ft(x_dict, 0.5)

            elif i == len(self.gat_layers)-1:
                x_dict = self.gat_layers[i](hg, x_dict)

        logits = {}
        embd = {}
        for k in self.label_keys:
            embd[k] = x_dict[k]
            logits[k] = torch.mm(x_dict[k], self.embd2class[k]) + self.bias[k]
            logits[k] = F.log_softmax(logits[k], dim=1)
        return logits

    def non_linear(self, x_dict):
        y_dict = {}
        for k in x_dict:
            y_dict[k] = F.elu(x_dict[k])
        return y_dict

    def dropout_ft(self, x_dict, dropout):
        y_dict = {}
        for k in x_dict:
            y_dict[k] = F.dropout(x_dict[k], dropout, training=self.training)
        return y_dict


class HeteGCNLayer(nn.Module):
    r"""
    Description
    -----------
    The calculating flow based on the network schema.

    Parameters
    ----------
    net_schema : dict
        the dgl graph's network schema generated by adjacency matrix
    in_layer_shape: dict
        the size of input features
    out_layer_shape: dict
        the size of output features
    type_fusion : string
        determine the calculation method for Type-level aggregation
    type_att_size :
        the attention size of type-level aggregation when using attention method.
    """

    def __init__(self, net_schema, in_layer_shape, out_layer_shape, type_fusion, type_att_size):
        super(HeteGCNLayer, self).__init__()

        self.net_schema = net_schema
        self.in_layer_shape = in_layer_shape
        self.out_layer_shape = out_layer_shape

        self.hete_agg = nn.ModuleDict()
        for k in net_schema:
            self.hete_agg[k] = HeteAggregateLayer(k, net_schema[k], in_layer_shape, out_layer_shape[k], type_fusion,
                                                  type_att_size)

    def forward(self, hg, x_dict):
        """
        Parameters
        ----------
        hg : object
            the dgl graph
        x_dict : dict
            dict for each type of features

        Returns
        -------
        ret_x_dict : dict
            The embeddings after aggregation.
        """

        ret_x_dict = {}
        for k in self.hete_agg.keys():
            ret_x_dict[k] = self.hete_agg[k](hg, x_dict)

        return ret_x_dict


class HeteAggregateLayer(nn.Module):
    r"""
    Description
    -----------
    The core part of ie-HGCN, the calculating flow of projection, object-level aggregation and type-level aggregation in
    a specific type block.

    Projection
        .. math::
            Y^{Self-\Omega }=H^{\Omega} \cdot W^{Self-\Omega}

            Y^{\Gamma - \Omega}=H^{\Gamma} \cdot W^{\Gamma - \Omega} , \Gamma \in N_{\Omega} (1)

    Object-level Aggregation
        .. math::
            Z^{ Self - \Omega } = Y^{ Self - \Omega}=H^{\Omega} \cdot W^{Self - \Omega}

            Z^{\Gamma - \Omega}=\hat{A}^{\Omega-\Gamma} \cdot Y^{\Gamma - \Omega} = \hat{A}^{\Omega-\Gamma} \cdot H^{\Gamma} \cdot W^{\Gamma - \Omega} (2)

    Type-level Aggregation
        .. math::
            Q^{\Omega}=Z^{Self-\Omega} \cdot W_q^{\Omega}

            K^{Self-\Omega}=Z^{Self -\Omega} \cdot W_{k}^{\Omega}

            K^{\Gamma - \Omega}=Z^{\Gamma - \Omega} \cdot W_{k}^{\Omega}, \quad \Gamma \in N_{\Omega} (3)

        .. math::
            e^{Self-\Omega}={ELU} ([K^{ Self-\Omega} \| Q^{\Omega}] \cdot w_{a}^{\Omega})

            e^{\Gamma - \Omega}={ELU} ([K^{\Gamma - \Omega} \| Q^{\Omega}] \cdot w_{a}^{\Omega}), \Gamma \in N_{\Omega} (4)

        .. math::
            [a^{Self-\Omega}\|a^{1 - \Omega}\| \ldots . a^{\Gamma - \Omega}\|\ldots\| a^{|N_{\Omega}| - \Omega}]=
            {softmax}([e^{Self - \Omega}\|e^{1 - \Omega}\| \ldots\|e^{\Gamma - \Omega}\| \ldots \| e^{|\N_{\Omega}| - \Omega}]) (5)

        .. math::
            H_{i,:}^{\Omega \prime}=\sigma(a_{i}^{Self-\Omega} \cdot Z_{i,:}^{Self-\Omega}+\sum_{\Gamma \in N_{\Omega}} a_{i}^{\Gamma - \Omega} \cdot Z_{i,:}^{\Gamma - \Omega}) (6)
    """
    def __init__(self,current_type, net_schema_src, in_layer_shape, out_layer_shape, type_fusion, type_att_size):
        super(HeteAggregateLayer, self).__init__()

        self.nb_list = net_schema_src
        self.current_type = current_type
        self.type_fusion = type_fusion
        self.in_layer_shape = in_layer_shape
        self.out_layer_shape = out_layer_shape

        self.W_rel = nn.ParameterDict()
        for k in self.nb_list:
            try:
                self.W_rel[k] = nn.Parameter(torch.FloatTensor(in_layer_shape[k], out_layer_shape))
            except KeyError as ke:
                self.W_rel[k] = nn.Parameter(torch.FloatTensor(in_layer_shape[self.current_type], out_layer_shape))
            finally:
                nn.init.xavier_uniform_(self.W_rel[k].data, gain=1.414)

        self.w_self = nn.Parameter(torch.FloatTensor(in_layer_shape[current_type], out_layer_shape))
        nn.init.xavier_uniform_(self.w_self.data, gain=1.414)
        self.bias = nn.Parameter(torch.FloatTensor(1, out_layer_shape))
        nn.init.xavier_uniform_(self.bias.data, gain=1.414)
        if type_fusion == 'att':
            self.w_query = nn.Parameter(torch.FloatTensor(out_layer_shape, type_att_size))
            nn.init.xavier_uniform_(self.w_query.data, gain=1.414)
            self.w_keys = nn.Parameter(torch.FloatTensor(out_layer_shape, type_att_size))
            nn.init.xavier_uniform_(self.w_keys.data, gain=1.414)
            self.w_att = nn.Parameter(torch.FloatTensor(type_att_size, 1))
            nn.init.xavier_uniform_(self.w_att.data, gain=1.414)

    def forward(self, hg:dgl.DGLGraph, x_dict):
        """
        Parameters
        ----------
        hg : object
            the dgl graph
        x_dict : dict
            the features of the graph
        Returns
        -------
        tensor
            The embeddings after aggregation.
        """
        # formula (1)
        hg.nodes[self.current_type].data['h'] = torch.mm(x_dict[self.current_type], self.w_self)
        nb_ft_list = [hg.nodes[self.current_type].data['h']]
        for k in self.nb_list:
            try:
                hg.nodes[k].data['h'] = torch.mm(x_dict[k],self.W_rel[k])
            finally:
                # formula (2)
                edge_type = k +"-"+self.current_type
                hg.nodes[self.current_type].data['m'] = torch.zeros_like(hg.nodes[self.current_type].data['h'])
                hg.send_and_recv(hg.edges(etype=edge_type), fn.copy_src('h', 't'),fn.sum('t', 'm'), etype=edge_type)
                nb_ft_list.append(hg.nodes[self.current_type].data['m'])
        if self.type_fusion == 'mean':
            # Step 3
            agg_nb_ft = torch.cat([nb_ft.unsqueeze(1) for nb_ft in nb_ft_list], 1).mean(1)

        elif self.type_fusion == 'att':
            # formula (3)
            att_query = torch.mm(hg.nodes[self.current_type].data['h'], self.w_query).repeat(len(nb_ft_list), 1)
            att_keys = torch.mm(torch.cat(nb_ft_list, 0), self.w_keys)
            att_input = att_query + att_keys
            att_input = F.dropout(att_input, 0.5, training=self.training)
            # formula (4)
            e = F.elu(torch.matmul(att_input, self.w_att))
            # formula (5)
            attention = F.softmax(e.view(len(nb_ft_list), -1).transpose(0, 1), dim=1)
            # formula (6)
            agg_nb_ft = torch.cat([nb_ft.unsqueeze(1) for nb_ft in nb_ft_list], 1).mul(attention.unsqueeze(-1)).sum(1)
        output = agg_nb_ft + self.bias
        return output
