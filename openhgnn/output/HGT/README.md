# Attention Network

| Model| Paper|
|:-----:|:-----:|
|HGT(WWW 2019)| [Heterogeneous Graph Transformer](https://arxiv.org/abs/2003.01332)|
|SimpleHGN(KDD 2021)|[Are we really making much progress? Revisiting, benchmarking,and refining heterogeneous graph neural networks](https://dl.acm.org/doi/pdf/10.1145/3447548.3467350)|
|HetSANN(AAAI 2020)|[An Attention-Based Graph Neural Network for Heterogeneous Structural Learning](https://arxiv.org/abs/1912.10832)|
|ieHGCN(TKDE 2021)|[Interpretable and Efficient Heterogeneous Graph Convolutional Network](https://arxiv.org/pdf/2005.13183.pdf)|

## Attention mechanism
This part, we will give the definition of attention methanism based on **GAT** and **Transformer**.

- In [GAT](https://arxiv.org/abs/1710.10903), it defined the attentional mechanism. A shared linear transformation, parametrized by a weight matrix, $W\in\mathcal{R}^{F^{'}\times F}$, is applied to every node. Then use a shared attentional mechanism $a: \mathcal{R}^{F^{'}}\times \mathcal{R}^{F}\rightarrow \mathcal{R}$ to compute *attention coefficients*:

$$
e_{ij} = a(Wh_i, Wh_j)
$$

- this indicate the importance of node $j$'s features to node $i$. $a$ is a single-layer feedforward neural network. Finally we can normalize them across all choices of $j$ using the softmax function:

$$
\alpha_{ij} = softmax_j(e_{ij}) = \frac{\text{exp}(e_{ij})}{\sum_{k\in \mathcal{N}_i} \text{exp}(e_{ik})}
$$

- In [Transformer](https://arxiv.org/abs/1706.03762), an attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. e.g. Scaled Dot-Product Attention:

$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

## DGL API
This part, we will give DGL API we used. As DGL released 0.8.0 version, more API can support heterogeneous graph such as TypedLinear, HeteroLinear. So we will give some details of these APIs.

### [TypedLinear](https://docs.dgl.ai/generated/dgl.nn.pytorch.TypedLinear.html)

```python
class TypedLinear(in_size, out_size, num_types, regularizer=None, num_bases=None)
```
Apply linear transformation according to types.

Parameters:
- in_size(int): Input feature size.
- out_size(int): Output feature size.
- num_types(int): Number of types(node or edge).
- regularizer(str, optional): Which weight regularizer to use “basis” or “bdd”, default is **None**:

  - basis: basis-decomposition.
  - bdd: block-diagonal-decomposition.

- num_bases(int, optional): Number of bases. Needed when **regularizer** is specified. Typically smaller than **num_types**. Default: **None**.

```python
forward(x, x_type, sorted_by_type=False)
``` 
Parameters:
- x(tensor): The input tensor.
- x_type(tensor): 1D tensor storing the type of the element in **x**.
- sorted_by_type(boolean): Whether the inputs have been sorted by the types. Forward on pre-sorted inputs may be faster.

So this API can be used when we use **to_homogeneous** to convert a heterogeneous graph to a homogeneous graph.

### [HeteroLinear](https://docs.dgl.ai/generated/dgl.nn.pytorch.HeteroLinear.html)

```python
class HeteroLinear(in_size, out_size, bias=True)
```
Apply linear transformations on heterogeneous inputs.

Parameters:
- in_size(dict[key, int]): Input feature size for heterogeneous inputs. A key can be a string or a tuple of strings.
- out_size(int): Output feature size.
- bias(boolean): If **True**, learns a bias term.

```python
forward(feat)
```
Parameters:
- feat(dict[key, tensor]): Heterogeneous input features.

So this API can be used if we want to apply different linear transformations to different types.

### [HeteroGraphConv](https://docs.dgl.ai/generated/dgl.nn.pytorch.HeteroGraphConv.html)

```python
class HeteroGraphConv(mods, aggregate='sum')
```
The heterograph convolution applies sub-modules on their associating relation graphs, which reads the features from source nodes and writes the updated ones to destination nodes. If multiple relations have the same destination node types, their results are aggregated by the specified method. If the relation graph has no edge, the corresponding module will not be called.

Parameters:
- mods(dict[str, nn.Module]): Modules associated with every edge types.
- aggregate (str, callable, optional): Method for aggregating node features generated by different relations. Allowed string values are ‘sum’, ‘max’, ‘min’, ‘mean’, ‘stack’. User can also customize the aggregator by providing a callable instance.

```python
forward(g, inputs, mod_args=None, mod_kwargs=None)
```
Parameters:
- g(DGLHeteroGraph) – Graph data.
- inputs(dict[str, Tensor] or pair of dict[str, Tensor]) – Input node features.

So this API can be used when we need to get relation subgraphs and apply nn.Module to each subgraph.

## Typical model

Based on HeteroGraphConv, we divide the attention model into two categories: Direct-Aggregation models and Dual-Aggregation models.
### Direct-Aggregation models

| Model| Attention coefficient |
|:-----:|:-----:|
|HGT|$W_{Q_{\phi{(s)}}}h_s W^{ATT}_{\psi{(r)}}(W_{K_{\phi{(t)}}}h_t)^T$|
|SimpleHGN|$LeakyReLU(a^T[Wh_s \parallel Wh_t \parallel W_r r_{\psi(<s,t>)}])$|
|HetSANN|$LeakyReLU([W_{\phi(t),\phi(s)} h_s\parallel W_{\phi(t),\phi(s)} h_t]a_r)$|

These models only have one aggregation process and do not distinguish between types of edges when aggregating, so they are not suitable for HeteroGraphConv.


### Dual-aggregation model

#### ieHGCN

| Model| Attention coefficient |
|:-----:|:-----:|
|ieHGCN|$ELU(a^T_{\phi(s)}[W_{Q_{\phi(s)}}h_s\parallel W_{K_{\phi(t)}}h_t])$|

This model has two aggregation process and distinguish between types of edges when aggregating, so this is suitable for HeteroGraphConv.
## Implement Details

### Direct-Aggregation models
- We first implement the convolution layer of the model SimpleHGN, and HetSANN. The convolutional layer of HGT we use is [hgtconv](https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.HGTConv.html?highlight=hgtconv#dgl.nn.pytorch.conv.HGTConv). The **\_\_init\_\_** parameters can be different as the models need different parameters.
The parameters of the **forward** part are the same: `g` is the homogeneous graph, `h` is the features, `ntype` denotes the type of each node, `etype` denotes the type of each edge, `presorted` tells if the `ntype` or `etype` is presorted to use [TypedLinear](https://docs.dgl.ai/generated/dgl.nn.pytorch.TypedLinear.html) in **dgl.nn** conveniently. If we use [dgl.to_homogeneous](https://docs.dgl.ai/generated/dgl.to_homogeneous.html?highlight=to_homogeneous#dgl.to_homogeneous) to get the features, the features are presorted.

- Then, we use the convolution layers to implement corresponding models. We need [dgl.to_homogeneous](https://docs.dgl.ai/generated/dgl.to_homogeneous.html?highlight=to_homogeneous#dgl.to_homogeneous) to get a homogeneous graph as when we use [edge_softmax](https://docs.dgl.ai/generated/dgl.nn.functional.edge_softmax.html?highlight=edge_softmax), we put all the edges together to calculate the attention coefficient instead of distinguishing the type of edges. 
- After passing the convolution layers, we need to convert the output features to a feature dictionary in a heterogeneous graph. We designed a tool in **openhgnn.utils.utils.py** named **to_hetero_feat**. This is because we do not have a better solution to get a feature dictionay using **dgl**. We can only use [dgl.to_heterogeneous](https://docs.dgl.ai/generated/dgl.to_heterogeneous.html), but it has many additional operations to make the programs slowly. After we get the feature dictionary, the model is complete.

### Dual-Aggregation model

- We refer to the idea of the implementation of [dgl.nn.HeteroGraphConv](https://docs.dgl.ai/generated/dgl.nn.pytorch.HeteroGraphConv.html?highlight=heterographconv#dgl.nn.pytorch.HeteroGraphConv). We extract the relationship subgraph based on the edge type and complete the aggregation using the convoluntion layers. Then, to aggregate type-specific features across different relations we have to compute attention coefficients step by step.

## How to run

- Clone the Openhgnn-DGL

  ```bash
  # For node classification task
  # You may select model HGT, SimpleHGN, HetSANN
  python main.py -m HGT -t node_classification -d imdb4MAGNN -g 0 --use_best_config
  ```

  If you do not have gpu, set -gpu -1.

## Performance

#### Task: Node classification

Evaluation metric: Micro/Macro-F1

<table>
   <tr>
      <td></td>
      <td colspan="2" align="center">HGBn-ACM</td>
      <td colspan="2" align="center">acm4GTN</td>
      <td colspan="2" align="center">imdb4MAGNN</td>
      <td colspan="2" align="center">dblp4MAGNN</td>
   </tr>
   <tr>
      <td>Model</td>
      <td>Micro-F1</td>
      <td>Macro-F1</td>
      <td>Micro-F1</td>
      <td>Macro-F1</td>
      <td>Micro-F1</td>
      <td>Macro-F1</td>
      <td>Micro-F1</td>
      <td>Macro-F1</td>
   </tr>
   <tr>
      <td>HGT</td>
      <td>88.95</td>
      <td>89.18</td>
      <td>90.21</td>
      <td>90.24</td>
      <td>49.37</td>
      <td>49.18</td>
      <td>87.23</td>
      <td>86.46</td>
   </tr>
   <tr>
      <td>SimpleHGN</td>
      <td><b>92.27</b></td>
      <td><b>92.36</b></td>
      <td>89.27</td>
      <td>89.28</td>
      <td>52.25</td>
      <td>48.78</td>
      <td>87.72</td>
      <td>87.08</td>
   </tr>
   <tr>
      <td>HetSANN</td>
      <td>88.4</td>
      <td>88.7</td>
      <td>92.24</td>
      <td>92.31</td>
      <td>52.88</td>
      <td>47.44</td>
      <td><b>89.54</b></td>
      <td><b>90.24</b></td>
   </tr>
   <tr>
      <td>ie-HGCN</td>
      <td>91.71</td>
      <td>91.99</td>
      <td><b>92.47</b></td>
      <td><b>92.56</b></td>
      <td><b>55.03</b></td>
      <td><b>52.18</b></td>
      <td>88.36</td>
      <td>87.37</td>
   </tr>
   <tr>
      <td></td>
   </tr>
</table>

## TrainerFlow: [node classification flow](../../trainerflow/node_classification.py)

## Hyper-parameter specific to the model

You can modify the parameters [HGT], [SimpleHGN], [HetSANN], [ieHGCN] in openhgnn/config.ini. 
## More

#### Contirbutor

Yaoqi Liu[GAMMA LAB]

#### If you have any questions,

Submit an issue or email to [YaoqiLiu@bupt.edu.cn](mailto:YaoqiLiu@bupt.edu.cn).
